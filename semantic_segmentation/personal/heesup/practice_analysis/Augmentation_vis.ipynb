{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version: 1.4.0\n",
      "GPU 사용 가능 여부: True\n",
      "Tesla P40\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import label_accuracy_score\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 전처리를 위한 라이브러리\n",
    "from pycocotools.coco import COCO\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations import *\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations.augmentations import functional as F\n",
    "\n",
    "# 시각화를 위한 라이브러리\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "plt.rcParams['axes.grid'] = False\n",
    "\n",
    "print('pytorch version: {}'.format(torch.__version__))\n",
    "print('GPU 사용 가능 여부: {}'.format(torch.cuda.is_available()))\n",
    "\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"   # GPU 사용 가능 여부에 따라 device 정보 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16   # Mini-batch size\n",
    "num_epochs = 20\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed 고정\n",
    "random_seed = 21\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "# torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "dataset_path = '/opt/ml/input/data'\n",
    "anns_file_path = dataset_path + '/' + 'train.json'\n",
    "\n",
    "# Read annotations\n",
    "with open(anns_file_path, 'r') as f:\n",
    "    dataset = json.loads(f.read())\n",
    "\n",
    "categories = dataset['categories']\n",
    "anns = dataset['annotations']\n",
    "imgs = dataset['images']\n",
    "nr_cats = len(categories)\n",
    "nr_annotations = len(anns)\n",
    "nr_images = len(imgs)\n",
    "\n",
    "# Load categories and super categories\n",
    "cat_names = []\n",
    "super_cat_names = []\n",
    "super_cat_ids = {}\n",
    "super_cat_last_name = ''\n",
    "nr_super_cats = 0\n",
    "for cat_it in categories:\n",
    "    cat_names.append(cat_it['name'])\n",
    "    super_cat_name = cat_it['supercategory']\n",
    "    # Adding new supercat\n",
    "    if super_cat_name != super_cat_last_name:\n",
    "        super_cat_names.append(super_cat_name)\n",
    "        super_cat_ids[super_cat_name] = nr_super_cats\n",
    "        super_cat_last_name = super_cat_name\n",
    "        nr_super_cats += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count annotations\n",
    "cat_histogram = np.zeros(nr_cats,dtype=int)\n",
    "for ann in anns:\n",
    "    cat_histogram[ann['category_id']] += 1\n",
    "\n",
    "# Initialize the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame({'Categories': cat_names, 'Number of annotations': cat_histogram})\n",
    "df = df.sort_values('Number of annotations', 0, False)\n",
    "\n",
    "# Plot the histogram\n",
    "plt.title(\"category distribution of train set \")\n",
    "plot_1 = sns.barplot(x=\"Number of annotations\", y=\"Categories\", data=df, label=\"Total\", color=\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category labeling \n",
    "sorted_temp_df = df.sort_index()\n",
    "\n",
    "# background = 0 에 해당되는 label 추가 후 기존들을 모두 label + 1 로 설정\n",
    "sorted_df = pd.DataFrame([\"Backgroud\"], columns = [\"Categories\"])\n",
    "sorted_df = sorted_df.append(sorted_temp_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class (Categories) 에 따른 index 확인 (0~11 : 총 12개)\n",
    "sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = list(sorted_df.Categories)\n",
    "\n",
    "def get_classname(classID, cats):\n",
    "    for i in range(len(cats)):\n",
    "        if cats[i]['id']==classID:\n",
    "            return cats[i]['name']\n",
    "    return \"None\"\n",
    "\n",
    "class CustomDataLoader(Dataset):\n",
    "    \"\"\"COCO format\"\"\"\n",
    "    def __init__(self, data_dir, mode = 'train', transform = None):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.coco = COCO(data_dir)\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        # dataset이 index되어 list처럼 동작\n",
    "        image_id = self.coco.getImgIds(imgIds=index)\n",
    "        image_infos = self.coco.loadImgs(image_id)[0]\n",
    "        \n",
    "        # cv2 를 활용하여 image 불러오기\n",
    "        paths = os.path.join(dataset_path, image_infos['file_name'])\n",
    "        images = cv2.imread(os.path.join(dataset_path, image_infos['file_name']))\n",
    "        images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.uint8)\n",
    "#         images /= 255.0\n",
    "        \n",
    "        if (self.mode in ('train', 'val')):\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=image_infos['id'])\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "            # Load the categories in a variable\n",
    "            cat_ids = self.coco.getCatIds()\n",
    "            cats = self.coco.loadCats(cat_ids)\n",
    "\n",
    "            # masks : size가 (height x width)인 2D\n",
    "            # 각각의 pixel 값에는 \"category id + 1\" 할당\n",
    "            # Background = 0\n",
    "            masks = np.zeros((image_infos[\"height\"], image_infos[\"width\"]))\n",
    "            # Unknown = 1, General trash = 2, ... , Cigarette = 11\n",
    "            for i in range(len(anns)):\n",
    "                className = get_classname(anns[i]['category_id'], cats)\n",
    "                pixel_value = category_names.index(className)\n",
    "                masks = np.maximum(self.coco.annToMask(anns[i])*pixel_value, masks)\n",
    "            masks = masks.astype(np.uint8)\n",
    "\n",
    "            # transform -> albumentations 라이브러리 활용\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform(image=images, mask=masks)\n",
    "                images = transformed[\"image\"]\n",
    "                masks = transformed[\"mask\"]\n",
    "            \n",
    "            return paths,images, masks, image_infos\n",
    "        \n",
    "        if self.mode == 'test':\n",
    "            # transform -> albumentations 라이브러리 활용\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform(image=images)\n",
    "                images = transformed[\"image\"]\n",
    "            \n",
    "            return paths, images, image_infos\n",
    "    \n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        # 전체 dataset의 size를 return\n",
    "        return len(self.coco.getImgIds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=4.28s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "train_path = dataset_path + '/train_all.json'\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_transform = A.Compose([\n",
    "                            ToTensorV2()\n",
    "                            ])\n",
    "\n",
    "train_dataset = CustomDataLoader(data_dir=train_path, mode='train', transform=train_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=1,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=4,\n",
    "                                           collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = []\n",
    "\n",
    "for step, (paths,images, masks, _) in enumerate(train_loader):\n",
    "    img_dir.append(paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "print(img_dir[0])\n",
    "\n",
    "Image.open(img_dir[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize작업 - test set정보를 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = dataset_path + '/test.json'\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "test_transform = A.Compose([\n",
    "                            ToTensorV2()\n",
    "                            ])\n",
    "\n",
    "test_dataset = CustomDataLoader(data_dir=test_path, mode='test', transform=test_transform)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=1,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=4,\n",
    "                                           collate_fn=collate_fn)\n",
    "\n",
    "img_dir = []\n",
    "\n",
    "for step, (paths,images, masks) in enumerate(test_loader):\n",
    "    img_dir.append(paths[0])\n",
    "    \n",
    "\n",
    "img_info = dict(heights=[], widths=[], means=[], stds=[])\n",
    "\n",
    "\n",
    "for path in tqdm(img_dir):\n",
    "    img = np.array(Image.open(path))\n",
    "    h, w, _ = img.shape\n",
    "    img_info['heights'].append(h)\n",
    "    img_info['widths'].append(w)\n",
    "    img_info['means'].append(img.mean(axis=(0,1)))\n",
    "    img_info['stds'].append(img.std(axis=(0,1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total number of people is {len(df)}')\n",
    "print(f'Total number of images is {len(df) * 7}')\n",
    "\n",
    "print(f'Minimum height for dataset is {np.min(img_info[\"heights\"])}')\n",
    "print(f'Maximum height for dataset is {np.max(img_info[\"heights\"])}')\n",
    "print(f'Average height for dataset is {int(np.mean(img_info[\"heights\"]))}')\n",
    "print(f'Minimum width for dataset is {np.min(img_info[\"widths\"])}')\n",
    "print(f'Maximum width for dataset is {np.max(img_info[\"widths\"])}')\n",
    "print(f'Average width for dataset is {int(np.mean(img_info[\"widths\"]))}')\n",
    "\n",
    "print(f'RGB Mean: {np.mean(img_info[\"means\"], axis=0) / 255.}')\n",
    "print(f'RGB Standard Deviation: {np.mean(img_info[\"stds\"], axis=0) / 255.}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = dataset_path + '/train_all.json'\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    Normalize(\n",
    "        mean=(0.46009655,0.43957878,0.41827092), \n",
    "        std=(0.2108204,0.20766491,0.21656131), \n",
    "        max_pixel_value=255.0, \n",
    "        p=1.0),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "train_dataset = CustomDataLoader(data_dir=train_path, mode='train', transform=train_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=1,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=4,\n",
    "                                           collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_images = []\n",
    "for step, (paths,images, masks, _) in enumerate(train_loader):\n",
    "    if step == 4:\n",
    "        break \n",
    "    temp_images.append(images[0])\n",
    "    \n",
    "    \n",
    "# temp_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(temp_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.min(temp_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.json / validation.json / test.json 디렉토리 설정\n",
    "train_path = dataset_path + '/train.json'\n",
    "\n",
    "# collate_fn needs for batch\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_transform = A.Compose([\n",
    "                            ToTensorV2()\n",
    "                            ])\n",
    "\n",
    "# create own Dataset 2\n",
    "# train dataset\n",
    "train_dataset = CustomDataLoader(data_dir=train_path, mode='train', transform=train_transform)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=4,\n",
    "                                           collate_fn=collate_fn)\n",
    "\n",
    "# train_loader의 output 결과(image 및 mask) 확인\n",
    "for _,imgs, masks, image_infos in train_loader:\n",
    "    image_infos = image_infos[0]\n",
    "    temp_images = imgs\n",
    "    temp_masks = masks\n",
    "    \n",
    "    break\n",
    "\n",
    "fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize=(12, 12))\n",
    "\n",
    "ax1.imshow(temp_images[0].permute([1,2,0]))\n",
    "ax1.grid(False)\n",
    "ax1.set_title(\"input image : {}\".format(image_infos['file_name']), fontsize = 15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.json / validation.json / test.json 디렉토리 설정\n",
    "train_path = dataset_path + '/train.json'\n",
    "\n",
    "# collate_fn needs for batch\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    CLAHE(p=1.0),\n",
    "    RandomResizedCrop(256,256),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# create own Dataset 2\n",
    "# train dataset\n",
    "train_dataset = CustomDataLoader(data_dir=train_path, mode='train', transform=train_transform)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=4,\n",
    "                                           collate_fn=collate_fn)\n",
    "\n",
    "# train_loader의 output 결과(image 및 mask) 확인\n",
    "for _,imgs, masks, image_infos in train_loader:\n",
    "    image_infos = image_infos[0]\n",
    "    temp_images = imgs\n",
    "    temp_masks = masks\n",
    "    \n",
    "    break\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12, 12))\n",
    "\n",
    "ax1.imshow(temp_images[0].permute([1,2,0]))\n",
    "ax1.grid(False)\n",
    "ax1.set_title(\"input image : {}\".format(image_infos['file_name']), fontsize = 15)\n",
    "\n",
    "ax2.imshow(temp_masks[0])\n",
    "ax2.grid(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.json / validation.json / test.json 디렉토리 설정\n",
    "train_path = dataset_path + '/train.json'\n",
    "\n",
    "# collate_fn needs for batch\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    VerticalFlip(p=1.0),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# create own Dataset 2\n",
    "# train dataset\n",
    "train_dataset = CustomDataLoader(data_dir=train_path, mode='train', transform=train_transform)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=4,\n",
    "                                           collate_fn=collate_fn)\n",
    "\n",
    "# train_loader의 output 결과(image 및 mask) 확인\n",
    "for _,imgs, masks, image_infos in train_loader:\n",
    "    image_infos = image_infos[0]\n",
    "    temp_images = imgs\n",
    "    temp_masks = masks\n",
    "    \n",
    "    break\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12, 12))\n",
    "\n",
    "ax1.imshow(temp_images[0].permute([1,2,0]))\n",
    "ax1.grid(False)\n",
    "ax1.set_title(\"input image : {}\".format(image_infos['file_name']), fontsize = 15)\n",
    "\n",
    "ax2.imshow(temp_masks[0])\n",
    "ax2.grid(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.json / validation.json / test.json 디렉토리 설정\n",
    "train_path = dataset_path + '/train.json'\n",
    "\n",
    "# collate_fn needs for batch\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    Blur(),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# create own Dataset 2\n",
    "# train dataset\n",
    "train_dataset = CustomDataLoader(data_dir=train_path, mode='train', transform=train_transform)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=4,\n",
    "                                           collate_fn=collate_fn)\n",
    "\n",
    "# train_loader의 output 결과(image 및 mask) 확인\n",
    "for _,imgs, masks, image_infos in train_loader:\n",
    "    image_infos = image_infos[0]\n",
    "    temp_images = imgs\n",
    "    temp_masks = masks\n",
    "    \n",
    "    break\n",
    "\n",
    "fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize=(12, 12))\n",
    "\n",
    "ax1.imshow(temp_images[0].permute([1,2,0]))\n",
    "ax1.grid(False)\n",
    "ax1.set_title(\"input image : {}\".format(image_infos['file_name']), fontsize = 15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.json / validation.json / test.json 디렉토리 설정\n",
    "train_path = dataset_path + '/train.json'\n",
    "\n",
    "# collate_fn needs for batch\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    ChannelDropout(),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# create own Dataset 2\n",
    "# train dataset\n",
    "train_dataset = CustomDataLoader(data_dir=train_path, mode='train', transform=train_transform)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=4,\n",
    "                                           collate_fn=collate_fn)\n",
    "\n",
    "# train_loader의 output 결과(image 및 mask) 확인\n",
    "for _,imgs, masks, image_infos in train_loader:\n",
    "    image_infos = image_infos[0]\n",
    "    temp_images = imgs\n",
    "    temp_masks = masks\n",
    "    \n",
    "    break\n",
    "\n",
    "fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize=(12, 12))\n",
    "\n",
    "ax1.imshow(temp_images[0].permute([1,2,0]))\n",
    "ax1.grid(False)\n",
    "ax1.set_title(\"input image : {}\".format(image_infos['file_name']), fontsize = 15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_transform = A.Compose([\n",
    "    ChannelDropout(),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "images = cv2.imread(\"/opt/ml/input/data/batch_02_vt/1500.jpg\")\n",
    "images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "images /= 255.0\n",
    "\n",
    "\n",
    "transformed = train_transform(image=images)\n",
    "images = transformed[\"image\"]\n",
    "\n",
    "fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize=(12, 12))\n",
    "\n",
    "ax1.imshow(images.permute([1,2,0]))\n",
    "ax1.grid(False)\n",
    "ax1.set_title(\"input image : {}\".format(image_infos['file_name']), fontsize = 15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    ChannelShuffle(),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "images = cv2.imread(\"/opt/ml/input/data/batch_02_vt/1500.jpg\")\n",
    "images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "images /= 255.0\n",
    "\n",
    "\n",
    "transformed = train_transform(image=images)\n",
    "images = transformed[\"image\"]\n",
    "\n",
    "fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize=(12, 12))\n",
    "\n",
    "ax1.imshow(images.permute([1,2,0]))\n",
    "ax1.grid(False)\n",
    "ax1.set_title(\"input image : {}\".format(image_infos['file_name']), fontsize = 15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLAHE는 작동안함 - 추후 unint8포멧으로 변경하여 해결\n",
    "train_transform = A.Compose([\n",
    "    CLAHE(p =1.0),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "images = cv2.imread(\"/opt/ml/input/data/batch_02_vt/1500.jpg\")\n",
    "images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "images /= 255.0\n",
    "\n",
    "\n",
    "transformed = train_transform(image=images)\n",
    "images = transformed[\"image\"]\n",
    "\n",
    "fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize=(12, 12))\n",
    "\n",
    "ax1.imshow(images.permute([1,2,0]))\n",
    "ax1.grid(False)\n",
    "ax1.set_title(\"input image : {}\".format(image_infos['file_name']), fontsize = 15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    CoarseDropout(max_holes=20, max_height=20,p =1.0),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "images = cv2.imread(\"/opt/ml/input/data/batch_02_vt/1500.jpg\")\n",
    "images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "images /= 255.0\n",
    "\n",
    "\n",
    "transformed = train_transform(image=images)\n",
    "images = transformed[\"image\"]\n",
    "\n",
    "fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize=(12, 12))\n",
    "\n",
    "ax1.imshow(images.permute([1,2,0]))\n",
    "ax1.grid(False)\n",
    "ax1.set_title(\"input image : {}\".format(image_infos['file_name']), fontsize = 15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    ColorJitter(p =1.0),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "images = cv2.imread(\"/opt/ml/input/data/batch_02_vt/1500.jpg\")\n",
    "images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "images /= 255.0\n",
    "\n",
    "\n",
    "transformed = train_transform(image=images)\n",
    "images = transformed[\"image\"]\n",
    "\n",
    "fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize=(12, 12))\n",
    "\n",
    "ax1.imshow(images.permute([1,2,0]))\n",
    "ax1.grid(False)\n",
    "ax1.set_title(\"input image : {}\".format(image_infos['file_name']), fontsize = 15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    Cutout(num_holes=10, max_h_size=30, max_w_size=30,p =1.0),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "images = cv2.imread(\"/opt/ml/input/data/batch_02_vt/1500.jpg\")\n",
    "images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "images /= 255.0\n",
    "\n",
    "\n",
    "transformed = train_transform(image=images)\n",
    "images = transformed[\"image\"]\n",
    "\n",
    "fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize=(12, 12))\n",
    "\n",
    "ax1.imshow(images.permute([1,2,0]))\n",
    "ax1.grid(False)\n",
    "ax1.set_title(\"input image : {}\".format(image_infos['file_name']), fontsize = 15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    Downscale(p =1.0),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "images = cv2.imread(\"/opt/ml/input/data/batch_02_vt/1500.jpg\")\n",
    "images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "images /= 255.0\n",
    "\n",
    "\n",
    "transformed = train_transform(image=images)\n",
    "images = transformed[\"image\"]\n",
    "\n",
    "fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize=(12, 12))\n",
    "\n",
    "ax1.imshow(images.permute([1,2,0]))\n",
    "ax1.grid(False)\n",
    "ax1.set_title(\"input image : {}\".format(image_infos['file_name']), fontsize = 15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    Flip(p =1.0),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "images = cv2.imread(\"/opt/ml/input/data/batch_02_vt/1500.jpg\")\n",
    "images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "images /= 255.0\n",
    "\n",
    "\n",
    "transformed = train_transform(image=images)\n",
    "images = transformed[\"image\"]\n",
    "\n",
    "fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize=(12, 12))\n",
    "\n",
    "ax1.imshow(images.permute([1,2,0]))\n",
    "ax1.grid(False)\n",
    "ax1.set_title(\"input image : {}\".format(image_infos['file_name']), fontsize = 15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    GaussianBlur(p =1.0),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "images = cv2.imread(\"/opt/ml/input/data/batch_02_vt/1500.jpg\")\n",
    "images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "images /= 255.0\n",
    "\n",
    "\n",
    "transformed = train_transform(image=images)\n",
    "images = transformed[\"image\"]\n",
    "\n",
    "fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize=(12, 12))\n",
    "\n",
    "ax1.imshow(images.permute([1,2,0]))\n",
    "ax1.grid(False)\n",
    "ax1.set_title(\"input image : {}\".format(image_infos['file_name']), fontsize = 15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    GaussianBlur(p =1.0),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "images = cv2.imread(\"/opt/ml/input/data/batch_02_vt/1500.jpg\")\n",
    "images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "images /= 255.0\n",
    "\n",
    "\n",
    "transformed = train_transform(image=images)\n",
    "images = transformed[\"image\"]\n",
    "print(type(images))\n",
    "print(images.shape)\n",
    "\n",
    "fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize=(12, 12))\n",
    "\n",
    "ax1.imshow(images.permute([1,2,0]))\n",
    "ax1.grid(False)\n",
    "ax1.set_title(\"input image : {}\".format(image_infos['file_name']), fontsize = 15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = cv2.imread(\"/opt/ml/input/data/batch_02_vt/1500.jpg\")\n",
    "images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "images /= 255.0\n",
    "\n",
    "images = augmentations.functional.add_snow(images,1,1.5)\n",
    "images = torch.tensor(images)\n",
    "print(images.shape)\n",
    "\n",
    "\n",
    "fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize=(12, 12))\n",
    "ax1.imshow(images)\n",
    "ax1.grid(False)\n",
    "ax1.set_title(\"input image : {}\".format(image_infos['file_name']), fontsize = 15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = cv2.imread(\"/opt/ml/input/data/batch_02_vt/1500.jpg\")\n",
    "images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "images /= 255.0\n",
    "\n",
    "rain_drops = ((100,100), (200,200))\n",
    "\n",
    "images = augmentations.functional.add_rain(\n",
    "    images,\n",
    "    slant = 10,\n",
    "    drop_length = 10,\n",
    "    drop_width = 3,\n",
    "    rain_drops = rain_drops,\n",
    "    blur_value = 1, \n",
    "    brightness_coefficient = 1.0,\n",
    "    drop_color = (200,200,200)\n",
    ")\n",
    "images = torch.tensor(images)\n",
    "print(images.shape)\n",
    "\n",
    "\n",
    "fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize=(12, 12))\n",
    "ax1.imshow(images)\n",
    "ax1.grid(False)\n",
    "ax1.set_title(\"input image : {}\".format(image_infos['file_name']), fontsize = 15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    ToGray(p = 1.0),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "images = cv2.imread(\"/opt/ml/input/data/batch_02_vt/1500.jpg\")\n",
    "images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "images /= 255.0\n",
    "\n",
    "\n",
    "transformed = train_transform(image=images)\n",
    "images = transformed[\"image\"]\n",
    "\n",
    "fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize=(12, 12))\n",
    "\n",
    "ax1.imshow(images.permute([1,2,0]))\n",
    "ax1.grid(False)\n",
    "ax1.set_title(\"input image : {}\".format(image_infos['file_name']), fontsize = 15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    Cutout(num_holes=10, \n",
    "                        max_h_size=int(.1 * 400), max_w_size=int(.1 * 400), \n",
    "                        p=1.0),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "images = cv2.imread(\"/opt/ml/input/data/batch_02_vt/1500.jpg\")\n",
    "images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "images /= 255.0\n",
    "\n",
    "\n",
    "transformed = train_transform(image=images)\n",
    "images = transformed[\"image\"]\n",
    "\n",
    "fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize=(12, 12))\n",
    "\n",
    "ax1.imshow(images.permute([1,2,0]))\n",
    "ax1.grid(False)\n",
    "ax1.set_title(\"input image : {}\".format(image_infos['file_name']), fontsize = 15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridMask Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridMask(DualTransform):\n",
    "    def __init__(self, num_grid=3, fill_value=0, rotate=0, mode=0, always_apply=False, p=0.5):\n",
    "        super(GridMask, self).__init__(always_apply, p)\n",
    "        if isinstance(num_grid, int):\n",
    "            num_grid = (num_grid, num_grid)\n",
    "        if isinstance(rotate, int):\n",
    "            rotate = (-rotate, rotate)\n",
    "        self.num_grid = num_grid\n",
    "        self.fill_value = fill_value\n",
    "        self.rotate = rotate\n",
    "        self.mode = mode\n",
    "        self.masks = None\n",
    "        self.rand_h_max = []\n",
    "        self.rand_w_max = []\n",
    "\n",
    "    def init_masks(self, height, width):\n",
    "        if self.masks is None:\n",
    "            self.masks = []\n",
    "            n_masks = self.num_grid[1] - self.num_grid[0] + 1\n",
    "            for n, n_g in enumerate(range(self.num_grid[0], self.num_grid[1] + 1, 1)):\n",
    "                grid_h = height / n_g\n",
    "                grid_w = width / n_g\n",
    "                this_mask = np.ones((int((n_g + 1) * grid_h), int((n_g + 1) * grid_w))).astype(np.uint8)\n",
    "                for i in range(n_g + 1):\n",
    "                    for j in range(n_g + 1):\n",
    "                        this_mask[\n",
    "                             int(i * grid_h) : int(i * grid_h + grid_h / 2),\n",
    "                             int(j * grid_w) : int(j * grid_w + grid_w / 2)\n",
    "                        ] = self.fill_value\n",
    "                        if self.mode == 2:\n",
    "                            this_mask[\n",
    "                                 int(i * grid_h + grid_h / 2) : int(i * grid_h + grid_h),\n",
    "                                 int(j * grid_w + grid_w / 2) : int(j * grid_w + grid_w)\n",
    "                            ] = self.fill_value\n",
    "                \n",
    "                if self.mode == 1:\n",
    "                    this_mask = 1 - this_mask\n",
    "\n",
    "                self.masks.append(this_mask)\n",
    "                self.rand_h_max.append(grid_h)\n",
    "                self.rand_w_max.append(grid_w)\n",
    "\n",
    "    def apply(self, image, mask, rand_h, rand_w, angle, **params):\n",
    "        h, w = image.shape[:2]\n",
    "        mask = F.rotate(mask, angle) if self.rotate[1] > 0 else mask\n",
    "        mask = mask[:,:,np.newaxis] if image.ndim == 3 else mask\n",
    "        image *= mask[rand_h:rand_h+h, rand_w:rand_w+w].astype(image.dtype)\n",
    "        return image\n",
    "\n",
    "    def get_params_dependent_on_targets(self, params):\n",
    "        img = params['image']\n",
    "        height, width = img.shape[:2]\n",
    "        self.init_masks(height, width)\n",
    "\n",
    "        mid = np.random.randint(len(self.masks))\n",
    "        mask = self.masks[mid]\n",
    "        rand_h = np.random.randint(self.rand_h_max[mid])\n",
    "        rand_w = np.random.randint(self.rand_w_max[mid])\n",
    "        angle = np.random.randint(self.rotate[0], self.rotate[1]) if self.rotate[1] > 0 else 0\n",
    "\n",
    "        return {'mask': mask, 'rand_h': rand_h, 'rand_w': rand_w, 'angle': angle}\n",
    "\n",
    "    @property\n",
    "    def targets_as_params(self):\n",
    "        return ['image']\n",
    "\n",
    "    def get_transform_init_args_names(self):\n",
    "        return ('num_grid', 'fill_value', 'rotate', 'mode')\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    GridMask(num_grid=3, p=1, rotate=0),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "images = cv2.imread(\"/opt/ml/input/data/batch_02_vt/1500.jpg\")\n",
    "images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "images /= 255.0\n",
    "\n",
    "\n",
    "transformed = train_transform(image=images)\n",
    "images = transformed[\"image\"]\n",
    "\n",
    "fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize=(12, 12))\n",
    "\n",
    "ax1.imshow(images.permute([1,2,0]))\n",
    "ax1.grid(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
